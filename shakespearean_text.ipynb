{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the dataset"
      ],
      "metadata": {
        "id": "RDf9lMj-3KM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLCHXhmi2mFT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcd1MXFc3Oy9",
        "outputId": "2b189be8-114d-4f2c-ee4f-ccdb8132d34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzbyjSQT3_1C",
        "outputId": "5cb9d693-cdc9-44a9-9700-5939ab89dda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use textVectorization to encode this text. SPlit it by character to get character level encoding rather than the default word level encoding."
      ],
      "metadata": {
        "id": "P8ke92ei4JmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
        "                                                   standardize=\"lower\") # make lowercase\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ],
      "metadata": {
        "id": "IYX8-UJy4UMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5sdqPq04gdU",
        "outputId": "ee5dbca4-b0c2-4589-8e69-9b8d18f01c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12])>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each character is now mapped to an integer starting at 2. The TextVectorization layer reserved the value 0 for padding tokens and it reserved 1 for unknown characters. We dont need either of these tokens for now, so lets subtract 2 from the character IDs and compute the number of distinct characters and total num of characters:"
      ],
      "metadata": {
        "id": "Ijxghj1r5Qy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2 # drop tokens 0 (pad) and 1(unknown), which we will not use for text generation\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2 # num of distinct chars = 39\n",
        "dataset_size = len(encoded) # total num of chars = 1,115,394"
      ],
      "metadata": {
        "id": "Euw5lUbT5u4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like all other NLP problems, can turn this very long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN.\n",
        "* The targets will be similar to inputs, but shifted by one time step into the future\n",
        "* For example, one sample in the dataset may be a sequence of character IDs representing the text \"to be or not to b\"(without final e), and the corresponding target - a sequence of character IDs with the text \"o be or not to be\"(with final e but without the leading t)\n",
        "\n",
        "Write a small utility function to convert a long sequence of character IDs into a dataset of input/target window parts\n"
      ],
      "metadata": {
        "id": "TJzOG4it6nlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "6lcNDq1s6UUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This funtions works:\n",
        "* takes a sequence as input (encoded text), creates a dataset containing all the windows of the desired length\n",
        "* increases the length by one, since we need the next character for the target\n",
        "* then it shuffles the windows(optionally), batches them, splits them into input/output pairs and activated prefetching"
      ],
      "metadata": {
        "id": "0QVAc6rT8P5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split set data\n",
        "90% of text for training, 5% for validation, 5% for testing"
      ],
      "metadata": {
        "id": "m5clJCZc8xaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
        "\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "rG95z9lx8tri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model\n",
        "\n",
        "Since the dataset is reasonably large, need more than a simple RNN, build and train a model with one GRU layer composed of 128 units."
      ],
      "metadata": {
        "id": "chn2VpFh9mCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint('my_shakespeare_model',\n",
        "                                                monitor=\"val_accuracy\",\n",
        "                                                save_best_only=True)\n",
        "\n",
        "history = model.fit(train_set,\n",
        "                    validation_data=valid_set,\n",
        "                    epochs=10,\n",
        "                    callbacks=[model_ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itwamkCg9zNi",
        "outputId": "9b133ff3-00ed-465a-8596-1f9ae4d62b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31247/31247 [==============================] - 439s 13ms/step - loss: 1.3990 - accuracy: 0.5721 - val_loss: 1.6137 - val_accuracy: 0.5315\n",
            "Epoch 2/10\n",
            "31247/31247 [==============================] - 410s 13ms/step - loss: 1.2961 - accuracy: 0.5965 - val_loss: 1.5883 - val_accuracy: 0.5383\n",
            "Epoch 3/10\n",
            "31247/31247 [==============================] - 406s 12ms/step - loss: 1.2773 - accuracy: 0.6007 - val_loss: 1.5772 - val_accuracy: 0.5411\n",
            "Epoch 4/10\n",
            "31247/31247 [==============================] - 401s 12ms/step - loss: 1.2677 - accuracy: 0.6027 - val_loss: 1.5726 - val_accuracy: 0.5419\n",
            "Epoch 5/10\n",
            "31247/31247 [==============================] - 403s 12ms/step - loss: 1.2610 - accuracy: 0.6039 - val_loss: 1.5685 - val_accuracy: 0.5450\n",
            "Epoch 6/10\n",
            "31247/31247 [==============================] - 401s 12ms/step - loss: 1.2561 - accuracy: 0.6050 - val_loss: 1.5635 - val_accuracy: 0.5464\n",
            "Epoch 7/10\n",
            "31247/31247 [==============================] - 392s 12ms/step - loss: 1.2523 - accuracy: 0.6059 - val_loss: 1.5569 - val_accuracy: 0.5475\n",
            "Epoch 8/10\n",
            "31247/31247 [==============================] - 397s 12ms/step - loss: 1.2500 - accuracy: 0.6064 - val_loss: 1.5640 - val_accuracy: 0.5453\n",
            "Epoch 9/10\n",
            "31247/31247 [==============================] - 420s 13ms/step - loss: 1.2470 - accuracy: 0.6072 - val_loss: 1.5637 - val_accuracy: 0.5457\n",
            "Epoch 10/10\n",
            "31247/31247 [==============================] - 414s 13ms/step - loss: 1.2452 - accuracy: 0.6074 - val_loss: 1.5595 - val_accuracy: 0.5486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),\n",
        "    model,\n",
        "])"
      ],
      "metadata": {
        "id": "_YTu7PmSQ5rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "y_pred = tf.argmax(y_proba) # choose most probable character ID\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "c5CkdFNtQ5g1",
        "outputId": "9c44d09c-b232-48dc-d9e6-a569cc593f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe model predicted the next character correctly. Lets use it to pretend we're shakespeare.\n",
        "\n",
        "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on.\n",
        "\n",
        "This is called greedy decoding. But in practice this leads to same words being repeated over and over.\n",
        "\n",
        "Instead we can sample the next character randomly with a probability equal to the estimated probability, using tensorflows tf.random.categorical() method. This will generate a more diverse and interesting text. the categorical function samples random class indices, given the class log probabilities(logits)."
      ],
      "metadata": {
        "id": "u8RF4LNsRgL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) # probas 50%, 40%, 10%\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8) # draw 8 samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DOBQ8-XRmw8",
        "outputId": "60735c63-3f96-481a-bd86-0777d9eb405e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 1, 1, 1, 0, 0, 0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have more control over the text, we can divide the logits by a number called the temperature, which we can tweak as we wish.\n",
        "\n",
        "A temp close to 0 favors high-probability chars while a high temp gives all chars a equal probability.\n",
        "\n",
        "* Lower temps are typically preferred when generating fairly rigid and precise text, such as  math equations\n",
        "* Higher temps are preffered when generating more diverse and creative text.\n",
        "\n",
        "The next_char function uses this approach to pick th next character to add to the input text:"
      ],
      "metadata": {
        "id": "jjN9NPCxS0ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "  return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "7tL6o6wXS0fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a helper function to repeatedly call next_char to get the next character and append it to the given text"
      ],
      "metadata": {
        "id": "_NVGeeUUUEQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "FhCuMEbwUDDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "print(extend_text(\"to be or not to be\", temperature=0.01))\n",
        "\n",
        "print(extend_text(\"to be or not to be\", temperature=1))\n",
        "\n",
        "print(extend_text(\"to be or not to be\", temperature=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thqmuMJiUae5",
        "outputId": "c6566750-1fcb-47cf-e9f2-48ab8469ac23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 231ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "to be or not to be a shame,\n",
            "and the duke is not the duke is not the \n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "to be or not to begin obs\n",
            "do i cannot be a shop father, it is\n",
            "resolv\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "to be or not to bepevicm-vilv!?$mz?gmjz :3?ljb'va;!td&\n",
            "i.ur3l'-j!3eu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shakespeare seems to be suffering from a heatwave from that last text. To generate a more convincing text, a common technique is to sample only from the top k characters, or only from the smallest set of top characters whose total probability exceeds some threshold(nucleas sampling).\n",
        "\n",
        "Alternatively you could try using beam search, or using more GRU layers and more neurons per layer, training for longer, and adding some regularization if needed.\n",
        "\n",
        "Also note the model is incapable of learning patterns longer than `length`, which is just 100 characters. You could try makeing this window larger but it will also make training harder, and even GRU and LSTM cells cannot handle very long sequences.\n",
        "\n",
        "An alternative is to use stateful RNN."
      ],
      "metadata": {
        "id": "DMtm756EVIWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stateful RNN\n",
        "\n",
        "Until now only used stateless RNNs: at each training iteration, the model starts with hidden state full of 0s, then it updates this state at each step, and after the last time step, it throws away as it is not needed anymore.\n",
        "\n",
        "What if we instructed the RNN to preserve this final state after processing a training batch and use it as the initial state for the next training batch? this way the model could learn long term patterns despite only backpropagating thru short sequences. This is called a **stateful RNN**\n",
        "\n"
      ],
      "metadata": {
        "id": "fSSDKOugWBvG"
      }
    }
  ]
}